[
["index.html", "Finn 6211 - Final Project Chapter 1 Introduction", " Finn 6211 - Final Project Davis Vaughan 2018-04-12 Chapter 1 Introduction This is the final project of the Finn 6211 class, with the intention of getting comfortable with spot rate data, their relation to yield curve factors, and various hedging strategies. The instructions for the report can be found here. An R package has been created to accompany the report. It contains a number of helper functions for cleaning data, manipulating the time series, and creating the hedging strategies. The package is named, ratekit and can be found on Github here. The book is intended to provide a summary of the methods used in the project. In each section of the book is a link to the script that was actually run to generate the results for the project. Those scripts are more in depth and cover every aspect of the project. This report was written with bookdown, a book authoring package for R. "],
["data.html", "Chapter 2 Data 2.1 Getting the data 2.2 Cleaning 2.3 Monthly and Ascending", " Chapter 2 Data 2.1 Getting the data Script) 01-download.R The data is retrieved from the Federal Reserve website, under the discussion series: The U.S. Treasury Yield Curve: 1961 to the Present. The link for that site is here. The specific data set that was downloaded was the XLS file included on that site. The data was immediately opened in Excel, and was resaved as an xlsx file. The format of the data is not a true xls file, instead it is some kind of xml file. This does not play nicely with R’s packages for importing Excel data, so a resave was necessary and is done manually. ratekit provides the download_rates_xls() helper function for this. 2.2 Cleaning Script) 02-cleaning.R Data is brought in using the readxl package and the ratekit helper, read_rates(). This function reads the rectangle of rates data only, and sets any -999.99 values to NA. These are often found through the dataset, and I assume they are meant to represent missing values. To visualize the NA values in the dataset, I use the visdat package. First, let’s look at what is immediately brought in by read_rates(). We will need a few packages throughout the chapter, so let’s load those now as well. library(visdat) library(ratekit) library(dplyr) library(tibbletime) raw &lt;- read_rates(&quot;data/raw/feds200628.xlsx&quot;) raw ## # A tibble: 14,163 x 100 ## date SVENY01 SVENY02 SVENY03 SVENY04 SVENY05 SVENY06 SVENY07 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2018-03-29 2.10 2.27 2.40 2.49 2.56 2.62 2.66 ## 2 2018-03-28 2.10 2.28 2.41 2.51 2.59 2.65 2.69 ## 3 2018-03-27 2.09 2.26 2.40 2.50 2.58 2.64 2.69 ## 4 2018-03-26 2.10 2.30 2.45 2.57 2.65 2.71 2.76 ## 5 2018-03-23 2.09 2.27 2.42 2.53 2.61 2.68 2.73 ## 6 2018-03-22 2.09 2.29 2.44 2.55 2.63 2.70 2.74 ## 7 2018-03-21 2.11 2.32 2.47 2.59 2.68 2.75 2.81 ## 8 2018-03-20 2.12 2.34 2.49 2.61 2.69 2.75 2.80 ## 9 2018-03-19 2.11 2.31 2.45 2.57 2.65 2.72 2.77 ## 10 2018-03-16 2.10 2.30 2.45 2.56 2.65 2.71 2.76 ## # ... with 14,153 more rows, and 92 more variables: SVENY08 &lt;dbl&gt;, ## # SVENY09 &lt;dbl&gt;, SVENY10 &lt;dbl&gt;, SVENY11 &lt;dbl&gt;, SVENY12 &lt;dbl&gt;, ## # SVENY13 &lt;dbl&gt;, SVENY14 &lt;dbl&gt;, SVENY15 &lt;dbl&gt;, SVENY16 &lt;dbl&gt;, ## # SVENY17 &lt;dbl&gt;, SVENY18 &lt;dbl&gt;, SVENY19 &lt;dbl&gt;, SVENY20 &lt;dbl&gt;, ## # SVENY21 &lt;dbl&gt;, SVENY22 &lt;dbl&gt;, SVENY23 &lt;dbl&gt;, SVENY24 &lt;dbl&gt;, ## # SVENY25 &lt;dbl&gt;, SVENY26 &lt;dbl&gt;, SVENY27 &lt;dbl&gt;, SVENY28 &lt;dbl&gt;, ## # SVENY29 &lt;dbl&gt;, SVENY30 &lt;dbl&gt;, SVENPY01 &lt;dbl&gt;, SVENPY02 &lt;dbl&gt;, ## # SVENPY03 &lt;dbl&gt;, SVENPY04 &lt;dbl&gt;, SVENPY05 &lt;dbl&gt;, SVENPY06 &lt;dbl&gt;, ## # SVENPY07 &lt;dbl&gt;, SVENPY08 &lt;dbl&gt;, SVENPY09 &lt;dbl&gt;, SVENPY10 &lt;dbl&gt;, ## # SVENPY11 &lt;dbl&gt;, SVENPY12 &lt;dbl&gt;, SVENPY13 &lt;dbl&gt;, SVENPY14 &lt;dbl&gt;, ## # SVENPY15 &lt;dbl&gt;, SVENPY16 &lt;dbl&gt;, SVENPY17 &lt;dbl&gt;, SVENPY18 &lt;dbl&gt;, ## # SVENPY19 &lt;dbl&gt;, SVENPY20 &lt;dbl&gt;, SVENPY21 &lt;dbl&gt;, SVENPY22 &lt;dbl&gt;, ## # SVENPY23 &lt;dbl&gt;, SVENPY24 &lt;dbl&gt;, SVENPY25 &lt;dbl&gt;, SVENPY26 &lt;dbl&gt;, ## # SVENPY27 &lt;dbl&gt;, SVENPY28 &lt;dbl&gt;, SVENPY29 &lt;dbl&gt;, SVENPY30 &lt;dbl&gt;, ## # SVENF01 &lt;dbl&gt;, SVENF02 &lt;dbl&gt;, SVENF03 &lt;dbl&gt;, SVENF04 &lt;dbl&gt;, ## # SVENF05 &lt;dbl&gt;, SVENF06 &lt;dbl&gt;, SVENF07 &lt;dbl&gt;, SVENF08 &lt;dbl&gt;, ## # SVENF09 &lt;dbl&gt;, SVENF10 &lt;dbl&gt;, SVENF11 &lt;dbl&gt;, SVENF12 &lt;dbl&gt;, ## # SVENF13 &lt;dbl&gt;, SVENF14 &lt;dbl&gt;, SVENF15 &lt;dbl&gt;, SVENF16 &lt;dbl&gt;, ## # SVENF17 &lt;dbl&gt;, SVENF18 &lt;dbl&gt;, SVENF19 &lt;dbl&gt;, SVENF20 &lt;dbl&gt;, ## # SVENF21 &lt;dbl&gt;, SVENF22 &lt;dbl&gt;, SVENF23 &lt;dbl&gt;, SVENF24 &lt;dbl&gt;, ## # SVENF25 &lt;dbl&gt;, SVENF26 &lt;dbl&gt;, SVENF27 &lt;dbl&gt;, SVENF28 &lt;dbl&gt;, ## # SVENF29 &lt;dbl&gt;, SVENF30 &lt;dbl&gt;, SVEN1F01 &lt;dbl&gt;, SVEN1F04 &lt;dbl&gt;, ## # SVEN1F09 &lt;dbl&gt;, BETA0 &lt;dbl&gt;, BETA1 &lt;dbl&gt;, BETA2 &lt;dbl&gt;, BETA3 &lt;dbl&gt;, ## # TAU1 &lt;dbl&gt;, TAU2 &lt;dbl&gt; Not a bad start, but I’m worried about missing values. Also, what are those column names? The column names in the data correspond to different types and lengths of rates used in the paper. The key for understanding the column names is below: series compounding_convention key Zero-coupon yield Continuously Compounded SVENYXX Par yield Coupon-Equivalent SVENPYXX Instantaneous forward rate Continuously Compounded SVENFXX One-year forward rate Coupon-Equivalent SVEN1FXX Parameters NA BETA0 to TAU2 Using vis_dat(), we can take a look at our dataset all at once to determine which data points to exclude. vis_dat(raw, warn_large_data = FALSE) A clear pattern is seen in the missing values, with the number of missing values increasing as you go further back in time and look at longer rates (10 year VS 30 year). This might be a bit difficult to see if you look at everything, but becomes clearer if you zoom in on just one set of series. The parameters are affected by this as well, but not as much, with only TAU2 being affected. Since the parameters are all we care about for this project, I decided to throw out any row with an NA value for TAU2. This threw out every data point before 1980. We can ensure that we don’t have any missing values now with vis_miss(). # This is the cleaned parameter set, cleaned using 02-cleaning.R parameters &lt;- readRDS(&quot;data/cleaned/parameters/parameters.rds&quot;) vis_miss(parameters) 2.3 Monthly and Ascending Script) 03-to-monthly-and-ascending.R At this point, our dataset looks like this: parameters &lt;- readRDS(&quot;data/cleaned/parameters/parameters.rds&quot;) parameters ## # A tibble: 9,543 x 7 ## date BETA0 BETA1 BETA2 BETA3 TAU1 TAU2 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2018-03-29 4.13 -2.25 0.000228 -3.07 2.84 11.4 ## 2 2018-03-28 4.35 -2.48 -0.0000890 -3.53 2.99 12.2 ## 3 2018-03-27 4.44 -2.59 0.000314 -3.67 3.18 12.5 ## 4 2018-03-26 4.27 -2.44 -0.0000588 -3.12 2.73 11.8 ## 5 2018-03-23 4.53 -2.69 0.000245 -3.78 3.17 12.9 ## 6 2018-03-22 4.31 -2.48 -0.000162 -3.31 2.78 11.9 ## 7 2018-03-21 4.80 -2.95 -0.550 -4.41 2.55 13.1 ## 8 2018-03-20 4.04 -2.21 -0.0000964 -2.43 2.43 10.8 ## 9 2018-03-19 4.66 -2.80 0.00297 -4.10 3.12 13.0 ## 10 2018-03-16 4.61 -2.78 0.000246 -4.02 3.03 12.9 ## # ... with 9,533 more rows We want monthly data, and we will need to put it in ascending order. We can convert to monthly with as_period() from tibbletime, and arrange it by ascending date with arrange() from dplyr. parameters_monthly &lt;- parameters %&gt;% as_tbl_time(date) %&gt;% arrange(date) %&gt;% as_period(&quot;monthly&quot;, side = &quot;end&quot;) parameters_monthly ## # A time tibble: 459 x 7 ## # Index: date ## date BETA0 BETA1 BETA2 BETA3 TAU1 TAU2 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1980-01-31 11.8 0.979 -622. 617. 2.50 2.50 ## 2 1980-02-29 11.8 1.53 -617. 621. 1.15 1.15 ## 3 1980-03-31 13.2 2.95 -622. 617. 1.78 1.76 ## 4 1980-04-30 11.1 0.607 -621. 617. 1.59 1.59 ## 5 1980-05-30 11.3 -3.44 -620. 618. 1.36 1.35 ## 6 1980-06-30 25.0 -17.0 -631. 607. 6.20 6.06 ## 7 1980-07-31 16.6 -8.33 -624. 615. 3.81 3.76 ## 8 1980-08-29 19.6 -8.94 -627. 612. 5.04 4.94 ## 9 1980-09-30 13.0 -1.34 -621. 618. 2.21 2.20 ## 10 1980-10-31 11.9 1.06 -618. 620. 0.379 0.379 ## # ... with 449 more rows This leaves us with 459 rows of data for our project, spanning 1980-01-31 to 2018-03-29. "],
["rates.html", "Chapter 3 Fixed Income Features Calculations 3.1 Spot Rates 3.2 Zero Coupon Bond Prices", " Chapter 3 Fixed Income Features Calculations In this chapter, I will construct spot rates, zero coupon bond prices, excess returns, and yield factors. These features will be used later in hedging strategies and for general exploration of the data. The following packages are required in this chapter. library(readr) library(ratekit) library(dplyr) 3.1 Spot Rates Script) 04-spot-rates-and-prices.R Spot rates series can be constructed from the 6 parameters in the rates dataset. The following formula is used to construct the spot rates. It is integrated form of the Svensson extension of the Nelson and Siegal approach to calculating instantaneous forward rates. Svensson added a second hump term to the model that Nelson and Siegal created. Integrating the instantaneous forward rates gives us the spot rates. To reconstruct this, I used a programming concept known as a function factory. This is a specialized function that returns a function. This extends naturally to this use case because the outer function can accept the time series of the 6 parameters, and the inner function that get’s returned is parameterized by n, corresponding to the n-year spot rate at time t. The spot_rate_factory() function lives in ratekit, and looks like this. spot_rate_factory ## function(beta_0, beta_1, beta_2, beta_3, tau_1, tau_2) { ## ## # Spot rate function based on Equation 22 of ## # Gurkaynak, Sack and Wright (2006) ## spot_rate_n &lt;- function(n) { ## beta_0 + ## beta_1 * (1 - exp( -n / tau_1)) / (n / tau_1) + ## beta_2 * ((1 - exp( -n / tau_1)) / (n / tau_1) - exp( -n / tau_1)) + ## beta_3 * ((1 - exp( -n / tau_2)) / (n / tau_2) - exp( -n / tau_2)) ## } ## ## spot_rate_n ## } ## &lt;environment: namespace:ratekit&gt; As you can see, it accepts the 6 parameters, and returns a function parameterized by n. Because we want to calculate the spot rate for a number of different years, this parameterized function will be very useful. Below is an example usage of this concept. # The monthly parameters from the Data chapter parameters_monthly &lt;- read_rds(&quot;data/cleaned/parameters/parameters_monthly.rds&quot;) # The generated function. The function signature is generate_spot_rates(n) generate_spot_rates &lt;- with( data = parameters_monthly, expr = spot_rate_factory(BETA0, BETA1, BETA2, BETA3, TAU1, TAU2) ) # Calculate the series of 1 year, 3 year, and 5 year spot rates parameters_monthly %&gt;% select(date) %&gt;% mutate( spot_1_yr = generate_spot_rates(1), spot_3_yr = generate_spot_rates(3), spot_5_yr = generate_spot_rates(5) ) ## # A time tibble: 459 x 4 ## # Index: date ## date spot_1_yr spot_3_yr spot_5_yr ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1980-01-31 11.8 10.9 10.6 ## 2 1980-02-29 14.5 13.2 12.4 ## 3 1980-03-31 15.1 13.1 12.1 ## 4 1980-04-30 10.7 10.1 10.2 ## 5 1980-05-30 8.71 9.10 9.41 ## 6 1980-06-30 8.53 9.10 9.35 ## 7 1980-07-31 9.08 9.74 9.95 ## 8 1980-08-29 11.1 11.3 11.2 ## 9 1980-09-30 11.8 11.6 11.4 ## 10 1980-10-31 13.0 12.3 12.2 ## # ... with 449 more rows 3.2 Zero Coupon Bond Prices Script) 04-spot-rates-and-prices.R n-year zero coupon bond prices can be calculated easily from their corresponding spot rates. Below is the relationship between the two. "],
["references.html", "References", " References "]
]
