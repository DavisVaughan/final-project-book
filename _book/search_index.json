[
["index.html", "Finn 6211 - Final Project Chapter 1 Introduction", " Finn 6211 - Final Project Davis Vaughan 2018-04-18 Chapter 1 Introduction This is the final project of the Finn 6211 class, with the intention of getting comfortable with spot rate data, their relation to yield curve factors, and various hedging strategies. The instructions for the report can be found here. An R package has been created to accompany the report. It contains a number of helper functions for cleaning data, manipulating the time series, and creating the hedging strategies. The package is named ratekit and can be found on Github here. The book is intended to provide a summary of the methods used in the project. In each section of the book is a link to the script that was actually run to generate the results for the project. Those scripts are more in depth and cover every aspect of the project. This report was written with bookdown, a book authoring package for R. "],
["data.html", "Chapter 2 Data 2.1 Getting the data 2.2 Cleaning 2.3 Monthly and Ascending", " Chapter 2 Data 2.1 Getting the data Script) 01-download.R The data is retrieved from the Federal Reserve website, under the discussion series: The U.S. Treasury Yield Curve: 1961 to the Present. The link for that site is here. The specific data set that was downloaded was the XLS file included on that site. The data was immediately opened in Excel, and was resaved as an xlsx file. The format of the data is not a true xls file, instead it is some kind of xml file. This does not play nicely with R’s packages for importing Excel data, so a resave was necessary and is done manually. ratekit provides the download_rates_xls() helper function for this. 2.2 Cleaning Script) 02-cleaning.R Data is brought in using the readxl package and the ratekit helper, read_rates(). This function reads the rectangle of rates data only, and sets any -999.99 values to NA. These are often found through the dataset, and I assume they are meant to represent missing values. To visualize the NA values in the dataset, I use the visdat package. First, let’s look at what is immediately brought in by read_rates(). We will need a few packages throughout the chapter, so let’s load those now as well. library(visdat) library(ratekit) library(dplyr) library(tibbletime) raw &lt;- read_rates(&quot;data/raw/feds200628.xlsx&quot;) raw ## # A tibble: 14,163 x 100 ## date SVENY01 SVENY02 SVENY03 SVENY04 SVENY05 SVENY06 SVENY07 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2018-03-29 2.10 2.27 2.40 2.49 2.56 2.62 2.66 ## 2 2018-03-28 2.10 2.28 2.41 2.51 2.59 2.65 2.69 ## 3 2018-03-27 2.09 2.26 2.40 2.50 2.58 2.64 2.69 ## 4 2018-03-26 2.10 2.30 2.45 2.57 2.65 2.71 2.76 ## 5 2018-03-23 2.09 2.27 2.42 2.53 2.61 2.68 2.73 ## # ... with 1.416e+04 more rows, and 92 more variables: SVENY08 &lt;dbl&gt;, ## # SVENY09 &lt;dbl&gt;, SVENY10 &lt;dbl&gt;, SVENY11 &lt;dbl&gt;, SVENY12 &lt;dbl&gt;, ## # SVENY13 &lt;dbl&gt;, SVENY14 &lt;dbl&gt;, SVENY15 &lt;dbl&gt;, SVENY16 &lt;dbl&gt;, ## # SVENY17 &lt;dbl&gt;, SVENY18 &lt;dbl&gt;, SVENY19 &lt;dbl&gt;, SVENY20 &lt;dbl&gt;, ## # SVENY21 &lt;dbl&gt;, SVENY22 &lt;dbl&gt;, SVENY23 &lt;dbl&gt;, SVENY24 &lt;dbl&gt;, ## # SVENY25 &lt;dbl&gt;, SVENY26 &lt;dbl&gt;, SVENY27 &lt;dbl&gt;, SVENY28 &lt;dbl&gt;, ## # SVENY29 &lt;dbl&gt;, SVENY30 &lt;dbl&gt;, SVENPY01 &lt;dbl&gt;, SVENPY02 &lt;dbl&gt;, ## # SVENPY03 &lt;dbl&gt;, SVENPY04 &lt;dbl&gt;, SVENPY05 &lt;dbl&gt;, SVENPY06 &lt;dbl&gt;, ## # SVENPY07 &lt;dbl&gt;, SVENPY08 &lt;dbl&gt;, SVENPY09 &lt;dbl&gt;, SVENPY10 &lt;dbl&gt;, ## # SVENPY11 &lt;dbl&gt;, SVENPY12 &lt;dbl&gt;, SVENPY13 &lt;dbl&gt;, SVENPY14 &lt;dbl&gt;, ## # SVENPY15 &lt;dbl&gt;, SVENPY16 &lt;dbl&gt;, SVENPY17 &lt;dbl&gt;, SVENPY18 &lt;dbl&gt;, ## # SVENPY19 &lt;dbl&gt;, SVENPY20 &lt;dbl&gt;, SVENPY21 &lt;dbl&gt;, SVENPY22 &lt;dbl&gt;, ## # SVENPY23 &lt;dbl&gt;, SVENPY24 &lt;dbl&gt;, SVENPY25 &lt;dbl&gt;, SVENPY26 &lt;dbl&gt;, ## # SVENPY27 &lt;dbl&gt;, SVENPY28 &lt;dbl&gt;, SVENPY29 &lt;dbl&gt;, SVENPY30 &lt;dbl&gt;, ## # SVENF01 &lt;dbl&gt;, SVENF02 &lt;dbl&gt;, SVENF03 &lt;dbl&gt;, SVENF04 &lt;dbl&gt;, ## # SVENF05 &lt;dbl&gt;, SVENF06 &lt;dbl&gt;, SVENF07 &lt;dbl&gt;, SVENF08 &lt;dbl&gt;, ## # SVENF09 &lt;dbl&gt;, SVENF10 &lt;dbl&gt;, SVENF11 &lt;dbl&gt;, SVENF12 &lt;dbl&gt;, ## # SVENF13 &lt;dbl&gt;, SVENF14 &lt;dbl&gt;, SVENF15 &lt;dbl&gt;, SVENF16 &lt;dbl&gt;, ## # SVENF17 &lt;dbl&gt;, SVENF18 &lt;dbl&gt;, SVENF19 &lt;dbl&gt;, SVENF20 &lt;dbl&gt;, ## # SVENF21 &lt;dbl&gt;, SVENF22 &lt;dbl&gt;, SVENF23 &lt;dbl&gt;, SVENF24 &lt;dbl&gt;, ## # SVENF25 &lt;dbl&gt;, SVENF26 &lt;dbl&gt;, SVENF27 &lt;dbl&gt;, SVENF28 &lt;dbl&gt;, ## # SVENF29 &lt;dbl&gt;, SVENF30 &lt;dbl&gt;, SVEN1F01 &lt;dbl&gt;, SVEN1F04 &lt;dbl&gt;, ## # SVEN1F09 &lt;dbl&gt;, BETA0 &lt;dbl&gt;, BETA1 &lt;dbl&gt;, BETA2 &lt;dbl&gt;, BETA3 &lt;dbl&gt;, ## # TAU1 &lt;dbl&gt;, TAU2 &lt;dbl&gt; Not a bad start, but I’m worried about missing values. Also, what are those column names? The column names in the data correspond to different types and lengths of rates used in the paper. The key for understanding the column names is below: series compounding_convention key Zero-coupon yield Continuously Compounded SVENYXX Par yield Coupon-Equivalent SVENPYXX Instantaneous forward rate Continuously Compounded SVENFXX One-year forward rate Coupon-Equivalent SVEN1FXX Parameters NA BETA0 to TAU2 Using vis_dat(), we can take a look at our dataset all at once to determine which data points to exclude. vis_dat(raw, warn_large_data = FALSE) A clear pattern is seen in the missing values, with the number of missing values increasing as you go further back in time and look at longer rates (10 year VS 30 year). This might be a bit difficult to see if you look at everything, but becomes clearer if you zoom in on just one set of series. The parameters are affected by this as well, but not as much, with only TAU2 being affected. Since the parameters are all we care about for this project, I decided to throw out any row with an NA value for TAU2. This threw out every data point before 1980. We can ensure that we don’t have any missing values now with vis_miss(). # This is the cleaned parameter set, cleaned using 02-cleaning.R parameters &lt;- readRDS(&quot;data/cleaned/parameters/parameters.rds&quot;) vis_miss(parameters) 2.3 Monthly and Ascending Script) 03-to-monthly-and-ascending.R At this point, our dataset looks like this: parameters &lt;- readRDS(&quot;data/cleaned/parameters/parameters.rds&quot;) parameters ## # A tibble: 9,543 x 7 ## date BETA0 BETA1 BETA2 BETA3 TAU1 TAU2 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2018-03-29 4.13 -2.25 0.000228 -3.07 2.84 11.4 ## 2 2018-03-28 4.35 -2.48 -0.000089 -3.53 2.99 12.2 ## 3 2018-03-27 4.44 -2.59 0.000314 -3.67 3.18 12.5 ## 4 2018-03-26 4.27 -2.44 -0.0000588 -3.12 2.73 11.8 ## 5 2018-03-23 4.53 -2.69 0.000245 -3.78 3.17 12.9 ## # ... with 9,538 more rows We want monthly data, and we will need to put it in ascending order. We can convert to monthly with as_period() from tibbletime, and arrange it by ascending date with arrange() from dplyr. parameters_monthly &lt;- parameters %&gt;% as_tbl_time(date) %&gt;% arrange(date) %&gt;% as_period(&quot;monthly&quot;, side = &quot;end&quot;) parameters_monthly ## # A time tibble: 459 x 7 ## # Index: date ## date BETA0 BETA1 BETA2 BETA3 TAU1 TAU2 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1980-01-31 11.8 0.979 -622. 617. 2.50 2.50 ## 2 1980-02-29 11.8 1.53 -617. 621. 1.15 1.15 ## 3 1980-03-31 13.2 2.95 -622. 617. 1.78 1.76 ## 4 1980-04-30 11.1 0.607 -621. 617. 1.59 1.59 ## 5 1980-05-30 11.3 -3.44 -620. 618. 1.36 1.35 ## # ... with 454 more rows This leaves us with 459 rows of data for our project, spanning 1980-01-31 to 2018-03-29. "],
["rates.html", "Chapter 3 Fixed Income Features Calculations 3.1 Spot Rates 3.2 Zero Coupon Bond Prices 3.3 One Month Returns 3.4 Excess Returns 3.5 Yield Curve Factors", " Chapter 3 Fixed Income Features Calculations In this chapter, I will construct spot rates, zero coupon bond prices, excess returns, and yield factors. These features will be used later in hedging strategies and for general exploration of the data. The following packages are required in this chapter. library(readr) library(ratekit) library(dplyr) 3.1 Spot Rates Script) 04-spot-rates-and-prices.R Spot rates series can be constructed from the 6 parameters in the rates dataset. The following formula is used to construct the spot rates. It is integrated form of the Svensson extension of the Nelson and Siegal approach to calculating instantaneous forward rates. Svensson added a second hump term to the model that Nelson and Siegal created. Integrating the instantaneous forward rates gives us the spot rates. To reconstruct this, I used a programming concept known as a function factory. This is a specialized function that returns a function. This extends naturally to this use case because the outer function can accept the time series of the 6 parameters, and the inner function that get’s returned is parameterized by n, corresponding to the n-year spot rate at time t. The spot_rate_factory() function lives in ratekit, and looks like this. spot_rate_factory ## function(beta_0, beta_1, beta_2, beta_3, tau_1, tau_2) { ## ## # Spot rate function based on Equation 22 of ## # Gurkaynak, Sack and Wright (2006) ## spot_rate_n &lt;- function(n) { ## ## spot_rate_percentage &lt;- ## beta_0 + ## beta_1 * (1 - exp( -n / tau_1)) / (n / tau_1) + ## beta_2 * ((1 - exp( -n / tau_1)) / (n / tau_1) - exp( -n / tau_1)) + ## beta_3 * ((1 - exp( -n / tau_2)) / (n / tau_2) - exp( -n / tau_2)) ## ## spot_rate_percentage / 100 ## } ## ## ## spot_rate_n ## } ## &lt;environment: namespace:ratekit&gt; As you can see, it accepts the 6 parameters, and returns a function parameterized by n. Because we want to calculate the spot rate for a number of different years, this parameterized function will be very useful. Below is an example usage of this concept. # The monthly parameters from the Data chapter parameters_monthly &lt;- read_rds(&quot;data/cleaned/parameters/parameters_monthly.rds&quot;) # The generated function. The function signature is generate_spot_rates(n) generate_spot_rates &lt;- with( data = parameters_monthly, expr = spot_rate_factory(BETA0, BETA1, BETA2, BETA3, TAU1, TAU2) ) # Calculate the series of 1/12 year, 11/12 year, and 1 year spot rates spot_rates &lt;- parameters_monthly %&gt;% select(date) %&gt;% mutate( spot_1_month = generate_spot_rates(1/12), spot_11_month = generate_spot_rates(11/12), spot_12_month = generate_spot_rates(1) ) spot_rates ## # A time tibble: 459 x 4 ## # Index: date ## date spot_1_month spot_11_month spot_12_month ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1980-01-31 0.126 0.118 0.118 ## 2 1980-02-29 0.136 0.145 0.145 ## 3 1980-03-31 0.161 0.152 0.151 ## 4 1980-04-30 0.116 0.107 0.107 ## 5 1980-05-30 0.0795 0.0867 0.0871 ## # ... with 454 more rows 3.2 Zero Coupon Bond Prices Script) 04-spot-rates-and-prices.R n-year zero coupon bond prices can be calculated easily from their corresponding spot rates. Below is the relationship between the two. Like for the spot rates, a function factory was constructed that accepted the spot rate function, and returned a function that calculates a vector of bond prices parameterized by n. zero_bond_price_factory ## function(spot_rate_fn) { ## ## # Zero coupon bond for 1 dollar is just discounted spot rate ## zero_bond_price_fn &lt;- function(n) { ## exp( - spot_rate_fn(n) * n) ## } ## ## zero_bond_price_fn ## } ## &lt;environment: namespace:ratekit&gt; Using this relationship, the zero coupon bond prices were computed as the following: generate_zero_prices &lt;- zero_bond_price_factory(generate_spot_rates) zero_prices &lt;- spot_rates %&gt;% transmute( date, zero_prices_1_month = generate_zero_prices(1/12), zero_prices_11_month = generate_zero_prices(11/12), zero_prices_12_month = generate_zero_prices(1) ) zero_prices ## # A time tibble: 459 x 4 ## # Index: date ## date zero_prices_1_month zero_prices_11_month zero_prices_12_month ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1980-01-31 0.990 0.897 0.889 ## 2 1980-02-29 0.989 0.875 0.865 ## 3 1980-03-31 0.987 0.870 0.860 ## 4 1980-04-30 0.990 0.906 0.899 ## 5 1980-05-30 0.993 0.924 0.917 ## # ... with 454 more rows 3.3 One Month Returns Script) 05-returns.R The time \\(t+\\Delta\\) return on a n-year bond is: Using the zero bond prices from before, it is easy to calculate returns. For example, 1 month returns for 1 year zero coupon bonds can be calculated as: returns &lt;- zero_prices %&gt;% mutate( zero_prices_12_month_lag = lag(zero_prices_12_month), one_month_return = zero_prices_11_month / zero_prices_12_month_lag - 1 ) %&gt;% select(-zero_prices_1_month, -zero_prices_12_month_lag) returns ## # A time tibble: 459 x 4 ## # Index: date ## date zero_prices_11_month zero_prices_12_month one_month_return ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1980-01-31 0.897 0.889 NA ## 2 1980-02-29 0.875 0.865 -0.0153 ## 3 1980-03-31 0.870 0.860 0.00574 ## 4 1980-04-30 0.906 0.899 0.0543 ## 5 1980-05-30 0.924 0.917 0.0275 ## # ... with 454 more rows 3.4 Excess Returns Script) 05-returns.R Excess returns are calculated over the 1 month treasury, specifically: For excess returns, the one month on the n-year bond must be calculated, and the return on the benchmark (1 month treasury) must be calculated. We already have 1-year bond returns, but we need to calculate our benchmark returns. That can be done using the same formula as the 1-year returns, but where \\(P_{t+\\Delta}(n-\\Delta) = 1\\), the maturity value. returns_bench &lt;- zero_prices %&gt;% mutate(return_benchmark = 1 / lag(zero_prices_1_month) - 1) %&gt;% select(date, zero_prices_1_month, return_benchmark) returns_bench ## # A time tibble: 459 x 3 ## # Index: date ## date zero_prices_1_month return_benchmark ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1980-01-31 0.990 NA ## 2 1980-02-29 0.989 0.0106 ## 3 1980-03-31 0.987 0.0114 ## 4 1980-04-30 0.990 0.0135 ## 5 1980-05-30 0.993 0.00973 ## # ... with 454 more rows With these two sets of returns in hand, we can calculate excess returns for the one year bond. excess_returns &lt;- returns %&gt;% left_join(returns_bench, &quot;date&quot;) %&gt;% transmute(date, excess_returns = one_month_return - return_benchmark) excess_returns ## # A time tibble: 459 x 2 ## # Index: date ## date excess_returns ## &lt;date&gt; &lt;dbl&gt; ## 1 1980-01-31 NA ## 2 1980-02-29 -0.0259 ## 3 1980-03-31 -0.00568 ## 4 1980-04-30 0.0408 ## 5 1980-05-30 0.0177 ## # ... with 454 more rows 3.5 Yield Curve Factors Script) 06-yield-curve-factors.R Finally, the yield curve factors, level, slope, and curvature are calculated as: The implementation of these is straightforward from the set of spot rates, so no example is shown here. "],
["q1.html", "Chapter 4 Question 1 4.1 Summary Statistics 4.2 Autocorrelations 4.3 Correlations 4.4 Time Series Visualizations", " Chapter 4 Question 1 Question: What are the time-series properties of the spot rates \\(y_t(1)\\), \\(y_t(5)\\), and \\(y_t(10)\\)? Report their summary statistics, including mean, standard deviation, skewness, kurtosis, and the first four autocorrelation coefficients, and the correlation matrix of the spot rates. Comment on your results. Also plot them and comment on the time series patterns. In this section, the following packages are used: library(readr) library(dplyr) library(tidyr) library(ratekit) library(ggplot2) library(broom) library(purrr) We will need the data for the \\(y_t(1)\\), \\(y_t(5)\\), and \\(y_t(10)\\) spot rates. These have already been calculated in the script referenced in 3.1, so we can just load them in. rates &lt;- read_rds(&quot;data/computed/rates.rds&quot;) n &lt;- c(&quot;1&quot;, &quot;5&quot;, &quot;10&quot;) spot_rates_q1 &lt;- rates %&gt;% filter(maturity_nm %in% n) spot_rates_q1 ## # A tibble: 1,377 x 4 ## maturity maturity_nm date spot_rate ## &lt;dbl&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; ## 1 1 1 1980-01-31 0.118 ## 2 1 1 1980-02-29 0.145 ## 3 1 1 1980-03-31 0.151 ## 4 1 1 1980-04-30 0.107 ## 5 1 1 1980-05-30 0.0871 ## # ... with 1,372 more rows 4.1 Summary Statistics Reported below are summary statistics on the three spot rate series. Maturity Mean Standard Deviation Kurtosis Skewness 1 0.0480538 0.0375029 2.913858 0.6505396 5 0.0566592 0.0345430 2.567191 0.5659543 10 0.0627654 0.0314939 2.586681 0.5793584 Unsurprisingly, the average spot rate increases with the maturity, but interestingly, the shorter maturity spot rates have higher volatility. The kurtosis of all three are less than that of a normal distribution, but the 1 year maturity is very close. All three are right-skewed, with longer right tails, which makes sense considering extremely high interest rate periods do happen, but are rare. 4.2 Autocorrelations The first four autocorrelation correlation coefficients of the 3 series are reported below, along with a plot of the ACF for the series. Each of the series are highly autocorrelated. Maturity Lag 1 Lag 2 Lag 3 Lag 4 1 0.9878979 0.9697433 0.9527499 0.9411647 5 0.9911143 0.9791933 0.9686252 0.9599084 10 0.9906205 0.9793246 0.9691974 0.9606667 By looking at the entire ACF, we can see that the amount of autocorrelation increases in maturity. At farther out lags, 1 is less autocorrelated than 5 and 5 less than 10. Figure 4.1: ACF for the 1, 5, and 10 year spot rates 4.3 Correlations Moving on to correlations, it is clear that the the series are highly correlated. This should not be surprising whatsoever. Intuitively, the 1 year is more correlated with the 5 year than with the 10 year. 1 5 10 1 1.0000000 0.9783620 0.9539364 5 0.9783620 1.0000000 0.9932312 10 0.9539364 0.9932312 1.0000000 4.4 Time Series Visualizations A look at the time series of the three series confirms the highly autocorrelated and correlated nature of the three series. 1 year spot rates are almost always below the longer maturity rates, as one would expect. Since 2010, 1 year spot rates have been incredibly low, but have started to pick back up in the last few years. Figure 4.2: A look at the 1, 5, and 10 year spot rates over time "],
["q2.html", "Chapter 5 Question 2 5.1 Regression 5.2 One Year Spot Rate 5.3 Five Year Spot Rate 5.4 Ten Year Spot Rate 5.5 Decomposing the Spot Curve 5.6 Coefficient Stability 5.7 Do We Need All Three Factors?", " Chapter 5 Question 2 Question: Can the three yield curve factors explain the time-series variation in spot rates? Regress \\(y_t(1)\\) on a constant and \\(X_t\\) and comment on the regression statistics. Perform the same analysis for \\(y_t(5)\\) and \\(y_t(10)\\). In this section, the following packages are used: library(readr) library(dplyr) library(tidyr) library(ratekit) library(ggplot2) library(broom) library(purrr) library(furrr) library(rsample) library(tibbletime) library(forcats) We will need the data for the \\(y_t(1)\\), \\(y_t(5)\\), and \\(y_t(10)\\) spot rates along with the yield curve factors. These have already been calculated in the scripts referenced in 3.1 and 3.5 so we can just load them in. rates &lt;- read_rds(&quot;data/computed/rates.rds&quot;) yield_curve_factors &lt;- read_rds(&quot;data/computed/yield_curve_factors.rds&quot;) n &lt;- c(&quot;1&quot;, &quot;5&quot;, &quot;10&quot;) spot_rates_q2 &lt;- rates spot_rates_q2 ## # A tibble: 6,426 x 4 ## maturity maturity_nm date spot_rate ## &lt;dbl&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; ## 1 0.0833 1/12 1980-01-31 0.126 ## 2 0.0833 1/12 1980-02-29 0.136 ## 3 0.0833 1/12 1980-03-31 0.161 ## 4 0.0833 1/12 1980-04-30 0.116 ## 5 0.0833 1/12 1980-05-30 0.0795 ## # ... with 6,421 more rows 5.1 Regression Using the concept of multiple models from the book, R 4 Data Science, implementing these regressions in R is incredibly straightforward. First, we shard the series into separate data frames. nested_spot &lt;- spot_rates_q2 %&gt;% # Add on the curve factors left_join(yield_curve_factors, by = &quot;date&quot;) %&gt;% # Group by maturity and nest group_by(maturity) %&gt;% select(-maturity_nm) %&gt;% nest() Then we apply the linear model to each shard. The far right column, model contains the results of the linear models. I went ahead and calculated models for every maturity because I am going to use them in some exploration later. nested_models &lt;- nested_spot %&gt;% mutate(model = map(data, ~ lm(spot_rate ~ level + slope + curvature, data = .x))) nested_models ## # A tibble: 14 x 3 ## maturity data model ## &lt;dbl&gt; &lt;list&gt; &lt;list&gt; ## 1 0.0833 &lt;tibble [459 × 5]&gt; &lt;S3: lm&gt; ## 2 0.25 &lt;tibble [459 × 5]&gt; &lt;S3: lm&gt; ## 3 0.917 &lt;tibble [459 × 5]&gt; &lt;S3: lm&gt; ## 4 1 &lt;tibble [459 × 5]&gt; &lt;S3: lm&gt; ## 5 2 &lt;tibble [459 × 5]&gt; &lt;S3: lm&gt; ## 6 2.92 &lt;tibble [459 × 5]&gt; &lt;S3: lm&gt; ## 7 3 &lt;tibble [459 × 5]&gt; &lt;S3: lm&gt; ## 8 4.92 &lt;tibble [459 × 5]&gt; &lt;S3: lm&gt; ## 9 5 &lt;tibble [459 × 5]&gt; &lt;S3: lm&gt; ## 10 6.92 &lt;tibble [459 × 5]&gt; &lt;S3: lm&gt; ## 11 7 &lt;tibble [459 × 5]&gt; &lt;S3: lm&gt; ## 12 8 &lt;tibble [459 × 5]&gt; &lt;S3: lm&gt; ## 13 9.92 &lt;tibble [459 × 5]&gt; &lt;S3: lm&gt; ## 14 10 &lt;tibble [459 × 5]&gt; &lt;S3: lm&gt; 5.2 One Year Spot Rate As you can see below, all estimates for the 1 year spot rate model are highly significant, and the Adjusted \\(R^2\\) is nearing 100%, suggesting that the model can explain essentially all of the variation in the spot rate. Term Estimate Standard Error Statistic P-Value (Intercept) 0.001014021 0.0001280165 7.921022 1.8e-14 level 0.989941953 0.0015226534 650.142657 0.0e+00 slope 0.264774017 0.0034323179 77.141460 0.0e+00 curvature -0.428350107 0.0057618892 -74.341954 0.0e+00 R Squared R Squared Adj Residual Std Error 0.9995224 0.9995193 0.0008223 A chart of the realized VS predicted time series confirms how well the variation is explained. It is important to remember that this model is not predicting future rates, and is simply used to gather intuition about past rates. plot_rates_vs_predictions &lt;- function(.rates, .model, .maturity) { .rates %&gt;% filter(maturity == .maturity) %&gt;% rename(Realized = spot_rate) %&gt;% mutate(Predicted = predict(.model)) %&gt;% gather(&quot;Realized/Predicted&quot;, &quot;Spot Rate&quot;, Realized, Predicted) %&gt;% ggplot(aes(x = date, y = `Spot Rate`, color = `Realized/Predicted`)) + geom_line() + theme_minimal() + scale_color_brewer(palette = &quot;Dark2&quot;) + labs(x = &quot;&quot;) } Figure 5.1: One year spot rate: In sample predictions VS realized 5.3 Five Year Spot Rate The model for the 5 year rate is similar to the 1 year rate in terms of explanatory power. Term Estimate Standard Error Statistic P-Value (Intercept) -0.001326488 0.0001178948 -11.25146 0 level 1.010756975 0.0014022642 720.80351 0 slope 0.862622600 0.0031609403 272.90063 0 curvature -0.237885069 0.0053063231 -44.83049 0 R Squared R Squared Adj Residual Std Error 0.9995226 0.9995194 0.0007572 Figure 5.2: Five year spot rate: In sample predictions VS realized 5.4 Ten Year Spot Rate And again, the 10 year model performs well too. Term Estimate Standard Error Statistic P-Value (Intercept) 0.001285415 9.632731e-05 13.34424 0 level 0.990119282 1.145736e-03 864.17722 0 slope 1.041806029 2.582683e-03 403.38126 0 curvature 0.101703126 4.335593e-03 23.45772 0 R Squared R Squared Adj Residual Std Error 0.9996166 0.9996141 0.0006187 Figure 5.3: Ten year spot rate: In sample predictions VS realized 5.5 Decomposing the Spot Curve Although not specifically asked for, I thought it might be interesting to decompose and plot the spot curve at a few particular points in time. We will need a few functions to do so. The functions essentially allow us to abstract away all the work so that we can just check out the spot rate graph at any date. extract_rates &lt;- function(.rates, .date) { .rates %&gt;% as_tbl_time(date) %&gt;% group_by(maturity) %&gt;% filter_time(~.date) } tidy_models &lt;- function(.nested_models, .date) { coefs &lt;- .nested_models %&gt;% mutate(coef = map(model, tidy)) %&gt;% unnest(coef) %&gt;% select(maturity, term, estimate) %&gt;% spread(term, estimate) %&gt;% select(-`(Intercept)`) yield_fct &lt;- yield_curve_factors %&gt;% as_tbl_time(date) %&gt;% filter_time(~.date) # Multiply the date&#39;s yield curve factors by the coefficients to get the # decomp of the term decomp &lt;- coefs %&gt;% mutate(level = level * yield_fct$level, slope = slope * yield_fct$slope, curvature = curvature * yield_fct$curvature) decomp } tidy_decomposed_spot_rate &lt;- function(.rates, .nested_models, .date) { .rates %&gt;% extract_rates(.date) %&gt;% left_join(tidy_models(.nested_models, .date), &quot;maturity&quot;) %&gt;% gather(&quot;line&quot;, &quot;value&quot;, -(maturity:date)) %&gt;% select(maturity, line, value) } What would these three functions work together to give? Well, for example, we can try looking at January of 2012. For that date, we get the spot curve values along with the level, slope and curvature components from each maturity’s model. rates %&gt;% tidy_decomposed_spot_rate(nested_models, &quot;2012-01&quot;) %&gt;% spread(line, value) ## # A tibble: 14 x 5 ## maturity curvature level slope spot_rate ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0833 2.89e- 3 0.00225 -1.04e- 3 0.00254 ## 2 0.25 2.51e-18 0.00225 1.25e-18 0.00225 ## 3 0.917 -5.37e- 3 0.00222 3.00e- 3 0.00158 ## 4 1 -5.65e- 3 0.00222 3.30e- 3 0.00155 ## 5 2 -6.59e- 3 0.00225 6.23e- 3 0.00188 ## 6 2.92 -5.83e- 3 0.00226 8.12e- 3 0.00307 ## 7 3 -5.73e- 3 0.00226 8.26e- 3 0.00320 ## 8 4.92 -3.24e- 3 0.00227 1.07e- 2 0.00723 ## 9 5 -3.14e- 3 0.00227 1.07e- 2 0.00743 ## 10 6.92 -9.69e- 4 0.00226 1.20e- 2 0.0121 ## 11 7 -8.88e- 4 0.00226 1.21e- 2 0.0123 ## 12 8 1.24e-17 0.00225 1.25e- 2 0.0147 ## 13 9.92 1.30e- 3 0.00223 1.30e- 2 0.0188 ## 14 10 1.34e- 3 0.00223 1.30e- 2 0.0190 This extends naturally to charting the decomposed spot rate. # A charting function with custom themes chart_decomposed_spot_rate &lt;- function(.decomposed_spot) { .decomposed_spot %&gt;% mutate(linetype = case_when( line == &quot;spot_rate&quot; ~ &quot;a&quot;, # this works by alphabetical. first is solid, then dashed TRUE ~ &quot;b&quot; )) %&gt;% ggplot(aes(x = maturity, y = value, color = line, linetype = linetype)) + geom_point() + geom_smooth(method = &quot;loess&quot;, se = FALSE) + theme_minimal() + scale_color_brewer(palette = &quot;Dark2&quot;) + labs(x = &quot;Maturity&quot;, y = &quot;Component Value&quot;, color = &quot;Component&quot;) + guides(linetype = FALSE) } Figure 5.4: Decomposed Spot Rate for January 2012 The spot rate in January of 1981 was definitely an interesting time period! It’s essentially inverted, with lower maturity bonds having higher spot rates than longer maturity bonds. Figure 5.5: Decomposed Spot Rate for January 1981 5.6 Coefficient Stability Another question worth asking is how stable the coefficients are throughout time. We can test this by running the same regression as before, but with a rolling window. This works by calculating the regression spot_rate ~ level + slope + curvature for the first 100 days, then shift forward 1 day and drop the last day, calculate the regression again, and repeat this for the length of the series. The rsample package provides a number of helpers for doing analysis exactly like this. First, let’s split up our data into the rolling subsets. This results in an extremely useful and compact format for our modeling purposes. all_splits &lt;- spot_rates_q2 %&gt;% # Add on the curve factors left_join(yield_curve_factors, by = &quot;date&quot;) %&gt;% group_by(maturity) %&gt;% select(-maturity_nm) %&gt;% nest() %&gt;% # For each maturity, split the data into subsets of 100 data points mutate(data_splits = map(data, ~rolling_origin(.x, initial = 100, assess = 1, cumulative = FALSE))) all_splits ## # A tibble: 14 x 3 ## maturity data data_splits ## &lt;dbl&gt; &lt;list&gt; &lt;list&gt; ## 1 0.0833 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; ## 2 0.25 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; ## 3 0.917 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; ## 4 1 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; ## 5 2 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; ## 6 2.92 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; ## 7 3 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; ## 8 4.92 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; ## 9 5 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; ## 10 6.92 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; ## 11 7 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; ## 12 8 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; ## 13 9.92 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; ## 14 10 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; If we were to dig into just 1 of the data_splits we would find another tibble containing the slices. all_splits$data_splits[[1]] ## # Rolling origin forecast resampling ## # A tibble: 359 x 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;S3: rsplit&gt; Slice001 ## 2 &lt;S3: rsplit&gt; Slice002 ## 3 &lt;S3: rsplit&gt; Slice003 ## 4 &lt;S3: rsplit&gt; Slice004 ## 5 &lt;S3: rsplit&gt; Slice005 ## # ... with 354 more rows Each of the rsplit objects here contain all of the info needed to run the linear model on that subset. We access that split’s 100 data points with analysis(). analysis(all_splits$data_splits[[1]]$splits[[1]]) ## # A tibble: 100 x 5 ## date spot_rate level slope curvature ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1980-01-31 0.126 0.124 -0.0176 0.00755 ## 2 1980-02-29 0.136 0.140 -0.0193 -0.0179 ## 3 1980-03-31 0.161 0.159 -0.0412 -0.00205 ## 4 1980-04-30 0.116 0.114 -0.00975 0.0137 ## 5 1980-05-30 0.0795 0.0816 0.0173 0.00118 ## # ... with 95 more rows This notation might all look complicated, but it turns out to be incredibly useful and scalable. To prove that, let’s run the model on every split, for every maturity. plan(multiprocess) all_splits &lt;- all_splits %&gt;% mutate( model_coef = future_map(data_splits, ~{ # For each maturity... maturity_splits &lt;- .x map_dfr(maturity_splits$splits, ~ { # For each split... split.x &lt;- .x date &lt;- assessment(split.x)$date # Run the model mod &lt;- lm(spot_rate ~ level + slope + curvature, data = analysis(split.x)) # Tidy up tidy_mod &lt;- mod %&gt;% tidy() mutate(tidy_mod, date = date) %&gt;% select(date, term, estimate) }) }) ) all_splits ## # A tibble: 14 x 4 ## maturity data data_splits model_coef ## &lt;dbl&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 0.0833 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; &lt;data.frame [1,436 × 3]&gt; ## 2 0.25 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; &lt;data.frame [1,436 × 3]&gt; ## 3 0.917 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; &lt;data.frame [1,436 × 3]&gt; ## 4 1 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; &lt;data.frame [1,436 × 3]&gt; ## 5 2 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; &lt;data.frame [1,436 × 3]&gt; ## 6 2.92 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; &lt;data.frame [1,436 × 3]&gt; ## 7 3 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; &lt;data.frame [1,436 × 3]&gt; ## 8 4.92 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; &lt;data.frame [1,436 × 3]&gt; ## 9 5 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; &lt;data.frame [1,436 × 3]&gt; ## 10 6.92 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; &lt;data.frame [1,436 × 3]&gt; ## 11 7 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; &lt;data.frame [1,436 × 3]&gt; ## 12 8 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; &lt;data.frame [1,436 × 3]&gt; ## 13 9.92 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; &lt;data.frame [1,436 × 3]&gt; ## 14 10 &lt;tibble [459 × 5]&gt; &lt;tibble [359 × 2]&gt; &lt;data.frame [1,436 × 3]&gt; We’ve just run 359 * 14 = 5026 linear models. For me, it runs in parallel on 4 cores in ~8 seconds. At this point, we can pick a maturity, and look at it’s coefficients over time. For example, the 1 year spot rate has fairly consistent model terms, but the curvature has begun to rise up from -0.5 to around -0.25. This change over time isn’t refected in the -0.428 curvature estimate we get from running the model over the full time period, and might offer other interesting insights. Figure 5.6: Coefficient stability for the 1 year spot rate using a 100 day rolling window The 5 year rate, on the other hand, shows the opposite result, with the curvature term decreasing over time. Figure 5.7: Coefficient stability for the 5 year spot rate using a 100 day rolling window Finally, the 10 year shows a similar trend as the 1 year, but starts higher initially. Figure 5.8: Coefficient stability for the 10 year spot rate using a 100 day rolling window 5.7 Do We Need All Three Factors? "],
["references.html", "References", " References "]
]
